https://medium.com/@ottaviocalzone/an-intuitive-explanation-of-lstm-a035eb6ab42c
[[Vanishing Gradients]]
- Activations are short-term memory
- Weights are long term memory

![[Pasted image 20250420130804.png]]

An LSTM unit receives three vectors (three lists of numbers) as input. Two vectors come from the LSTM itself and were generated by the LSTM at the previous instant (instant _t_ − 1). These are the cell state (_C_) and the hidden state (_H_). The third vector comes from outside. This is the vector _X_ (called input vector) submitted to the LSTM at instant _t_.

- ==cell state acts as a long-term memory, while the hidden state acts as a short-term memory==
- uses recent past information (the short-term memory, _H_) and new information coming from the outside (the input vector, _X_) to update the long-term memory (cell state, _C_)




![[Pasted image 20250420203055.png]]

See how each gate is a sigmoid? This is because the gates should be b/w 0 and 1 and act as "selectors" to another vector: they decide which components of the vector to keep, and by how much.

Each gate uses Xt and Ht-1. Those both have their own weight matrices associated with them.
For example, input gate It = sigmoid(XtWxi + Ht-1Whi + b)