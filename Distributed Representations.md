DL also takes advantage of another form of feature composition called distribution representations. If each hidden unit can be though of representing a feature, there can be combinations of these hidden units being activated that represent certain features. If a network is processing images of faces, each face could have glasses, a at, and a beard = 8 possible combinations of features, which can be represented within 3 hidden units.

#look_into Why do distributed representations make models better at handling noise?
All the information is not contained in a single connection, so....