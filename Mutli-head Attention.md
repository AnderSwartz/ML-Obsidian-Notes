Just like in a ConvNet when we had different kernels, [[Transformers]] have multi-head attention where have can have multiple, separate Q, K, V groups for the same sequence

This is why Q, K, and V becomes matrices. "A different vector for every head of attention"